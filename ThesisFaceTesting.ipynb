{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ThesisFaceTesting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7HbXqhtNA7UNTbmaFncU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaavyaastalin/AgeGenderEmotion/blob/master/ThesisFaceTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F24mofIBA-6x",
        "outputId": "8e000617-ebf7-46aa-dace-6d8103076d68"
      },
      "source": [
        "!git clone https://github.com/kaushikjadhav01/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 47 (delta 10), reused 26 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pWv1JkzIQ4ty",
        "outputId": "74052376-ccee-428d-9cbb-a2bccab4abc6"
      },
      "source": [
        "import os\n",
        "RESEARCH_DIR = \"/content/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System\"\n",
        "os.chdir(RESEARCH_DIR)\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inD_omUwRLz6",
        "outputId": "a8c4d59e-7da6-4319-f393-ea23c61a6608"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.1.2.30)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (19.18.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras->-r requirements.txt (line 1)) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCnPfs4P-39M"
      },
      "source": [
        "from pathlib import Path\n",
        "import cv2\n",
        "import dlib\n",
        "import sys\n",
        "import numpy as np\n",
        "import argparse\n",
        "from contextlib import contextmanager\n",
        "from wide_resnet import WideResNet\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "classifier = load_model('./model/emotion_little_vgg_2.h5')\n",
        "pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.5/weights.28-3.73.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcFp07ZS_OAf"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import json\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Define Image Path Here\n",
        "image_path = \"./images/\"\n",
        "\n",
        "modhash = 'fbe63257a054c1c5466cfd7bf14646d6'\n",
        "emotion_classes = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
        "\n",
        "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "               font_scale=0.8, thickness=1):\n",
        "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    x, y = point\n",
        "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
        "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
        "    \n",
        "\n",
        "# Define our model parameters\n",
        "depth = 16\n",
        "k = 8\n",
        "weight_file = None\n",
        "margin = 0.4\n",
        "image_dir = None\n",
        "\n",
        "# Get our weight file \n",
        "if not weight_file:\n",
        "    weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
        "                           file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\n",
        "# load model and weights\n",
        "img_size = 64\n",
        "model = WideResNet(img_size, depth=depth, k=k)()\n",
        "model.load_weights(weight_file)\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "image_names = [f for f in listdir(image_path) if isfile(join(image_path, f))]\n",
        "\n",
        "for image_name in image_names:\n",
        "    frame = cv2.imread(\"./images/\" + image_name)\n",
        "    preprocessed_faces_emo = []           \n",
        " \n",
        "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    img_h, img_w, _ = np.shape(input_img)\n",
        "    detected = detector(frame, 1)\n",
        "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
        "    \n",
        "    preprocessed_faces_emo = []\n",
        "    if len(detected) > 0:\n",
        "        for i, d in enumerate(detected):\n",
        "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
        "            xw1 = max(int(x1 - margin * w), 0)\n",
        "            yw1 = max(int(y1 - margin * h), 0)\n",
        "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
        "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
        "            face_gray_emo = img_to_array(face_gray_emo)\n",
        "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
        "            preprocessed_faces_emo.append(face_gray_emo)\n",
        "\n",
        "        # make a prediction for Age and Gender\n",
        "        results = model.predict(np.array(faces))\n",
        "        predicted_genders = results[0]\n",
        "        ages = np.arange(0, 101).reshape(101, 1)\n",
        "        predicted_ages = results[1].dot(ages).flatten()\n",
        "        #print(predicted_ages)\n",
        "\n",
        "        # make a prediction for Emotion \n",
        "        emo_labels = []\n",
        "        for i, d in enumerate(detected):\n",
        "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
        "            emo_labels.append(emotion_classes[preds.argmax()])\n",
        "            #print(emo_labels)\n",
        "        \n",
        "        # draw results\n",
        "        labels = []\n",
        "        with open('HumanDetails.csv', 'a', newline='', encoding='utf-8') as out_file:\n",
        "          for i, d in enumerate(detected):\n",
        "            #labels = []\n",
        "\n",
        "            \n",
        "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
        "                                        \"F\" if predicted_genders[i][0] > 0.4 else \"M\", emo_labels[i])\n",
        "            labels.append(label)\n",
        "            draw_label(frame, (d.left(), d.top()), label)\n",
        "            \n",
        "            #resultDF = pd.DataFrame(labels)\n",
        "          writer = csv.writer(out_file, delimiter=' ' )\n",
        "            #writer.writerow(('age', 'gender','emotion'))\n",
        "          writer.writerows(labels)\n",
        "           #resultDF.to_csv('NewFilue.csv')\n",
        "            #json.dump(labels, outfile)\n",
        "\n",
        "    #cv2.imshow(\"Emotion Detector\", frame)\n",
        "    filename = \"p/\"+image_name\n",
        "    cv2.imwrite(filename,frame)\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uSTcwdY-pFFx",
        "outputId": "faa57c6c-e26f-4b28-f143-1186cf190610"
      },
      "source": [
        "labels[0]\\t  quotechar='|', quoting=csv.QUOTE_MINIMAL"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'37, M, Happy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "ucpwHEt9S7mB",
        "outputId": "0ea6ff45-ce14-4004-d6b2-63b1db79e41f"
      },
      "source": [
        "with open('data1.txt', 'r') as in_file:\n",
        "    stripped = (line.strip() for line in in_file)\n",
        "    lines = (line.split(\",\") for line in stripped if line)\n",
        "    with open('log.csv', 'w', newline='') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        writer.writerow(('age', 'gender','emotion'))\n",
        "        writer.writerows(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-69beec88cff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0min_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstripped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstripped\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "Omis7T8fmy-9",
        "outputId": "286932ae-c786-482c-d481-d13af6a1d776"
      },
      "source": [
        "import pandas as pd\n",
        "sd = pd.read_csv('aaaa.csv', header=None)\n",
        "sd"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 9</td>\n",
              "      <td>\" \" M</td>\n",
              "      <td>\" \" N e u t r a l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4 7</td>\n",
              "      <td>\" \" M</td>\n",
              "      <td>\" \" S a d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 7</td>\n",
              "      <td>\" \" F</td>\n",
              "      <td>\" \" H a p p y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5 5</td>\n",
              "      <td>\" \" M</td>\n",
              "      <td>\" \" H a p p y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6 2</td>\n",
              "      <td>\" \" M</td>\n",
              "      <td>\" \" H a p p y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3 6</td>\n",
              "      <td>\" \" F</td>\n",
              "      <td>\" \" N e u t r a l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4 1</td>\n",
              "      <td>\" \" F</td>\n",
              "      <td>\" \" H a p p y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4 1</td>\n",
              "      <td>\" \" M</td>\n",
              "      <td>\" \" H a p p y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3 7</td>\n",
              "      <td>\" \" M</td>\n",
              "      <td>\" \" H a p p y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0        1                   2\n",
              "0  5 9    \" \" M    \" \" N e u t r a l\n",
              "1  4 7    \" \" M            \" \" S a d\n",
              "2  3 7    \" \" F        \" \" H a p p y\n",
              "3  5 5    \" \" M        \" \" H a p p y\n",
              "4  6 2    \" \" M        \" \" H a p p y\n",
              "5  3 6    \" \" F    \" \" N e u t r a l\n",
              "6  4 1    \" \" F        \" \" H a p p y\n",
              "7  4 1    \" \" M        \" \" H a p p y\n",
              "8  3 7    \" \" M        \" \" H a p p y"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "9Prb0KjQRdox",
        "outputId": "f799c152-bbda-4ceb-b53a-09e3218e6c28"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "modhash = 'fbe63257a054c1c5466cfd7bf14646d6'\n",
        "emotion_classes = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
        "\n",
        "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "               font_scale=0.8, thickness=1):\n",
        "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    x, y = point\n",
        "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
        "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
        "    \n",
        "\n",
        "# Define our model parameters\n",
        "depth = 16\n",
        "k = 8\n",
        "weight_file = None\n",
        "margin = 0.4\n",
        "image_dir = None\n",
        "\n",
        "# Get our weight file \n",
        "if not weight_file:\n",
        "    weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
        "                           file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\n",
        "# load model and weights\n",
        "img_size = 64\n",
        "model = WideResNet(img_size, depth=depth, k=k)()\n",
        "model.load_weights(weight_file)\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "# Initialize Webcam\n",
        "cap = cv2.VideoCapture('/content/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System/videos/PP84_T1_H.mp4')\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('output_video.mp4', fourcc, 10.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    \n",
        "\n",
        "    preprocessed_faces_emo = []           \n",
        " \n",
        "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    img_h, img_w, _ = np.shape(input_img)\n",
        "    detected = detector(frame, 1)\n",
        "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
        "    \n",
        "    preprocessed_faces_emo = []\n",
        "    if len(detected) > 0:\n",
        "        for i, d in enumerate(detected):\n",
        "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
        "            xw1 = max(int(x1 - margin * w), 0)\n",
        "            yw1 = max(int(y1 - margin * h), 0)\n",
        "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
        "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
        "            face_gray_emo = img_to_array(face_gray_emo)\n",
        "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
        "            preprocessed_faces_emo.append(face_gray_emo)\n",
        "\n",
        "        # make a prediction for Age and Gender\n",
        "        results = model.predict(np.array(faces))\n",
        "        predicted_genders = results[0]\n",
        "        ages = np.arange(0, 101).reshape(101, 1)\n",
        "        predicted_ages = results[1].dot(ages).flatten()\n",
        "\n",
        "        # make a prediction for Emotion \n",
        "        emo_labels = []\n",
        "        for i, d in enumerate(detected):\n",
        "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
        "            emo_labels.append(emotion_classes[preds.argmax()])\n",
        "        \n",
        "        # draw results\n",
        "        for i, d in enumerate(detected):\n",
        "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
        "                                        \"F\" if predicted_genders[i][0] > 0.4 else \"M\", emo_labels[i])\n",
        "            draw_label(frame, (d.left(), d.top()), label)\n",
        "\n",
        "    out.write(frame)\n",
        "    #cv2.imshow(\"Emotion Detector\", frame)\n",
        "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-41857c111256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpreprocessed_faces_emo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mimg_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mdetected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    }
  ]
}